{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Qual o objetivo do comando cache em Spark?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É salvar um RDD na memória da JVM, para que não precise ser recomputado posteriormente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> O mesmo código implementado em Spark é normalmente mais rápido que a implementação equivalente em MapReduce. Por quê?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porque o MapReduce usa o disco para guardar dados intermediários e porque, para algoritmos iterativos, é necessário calcular a mesma coisa várias vezes, o que torna as coisas lentas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Qual é a função do SparkContext?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O spark context é o ponto de entrada dos aplicativos spark, é através dele que você carrega e salva dados, seta configurações, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Explique com suas palavras  o que é Resilient Distributed Datasets (RDD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É a primitiva principal de dados do Spark - é como dados são ordenados em blocos para que seja possível armazenar e enviar blocos de dados de um nó para outro, dentro do cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> GroupByKey é menos eficiente que reduceByKey em grandes dataset. Por quê?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porque a parte que é agregada não é \"consolidada\" com uma funcão que leva dois valores a um só. Isso faz com que os valores correspondentes a cada \"key\" sejam todos retornados, o que pode usar muito espaço e quebrar o applicativo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Explique o que o código Scala abaixo faz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**val textFile = sc.textFile(\"hdfs://...\")**\n",
    "// carrega um arquivo texto do sistema de arquivos do hadoop (HDFS) como um RDD.\n",
    "\n",
    "\n",
    "**val counts = textFile.flatMap(line => line.split(\" \"))** // transforma cada linha do arquivo em uma lista de palavras\n",
    "\n",
    "**.map(word => (word, 1))** // transforma cada palavra em uma tupla, para contagem\n",
    "\n",
    "**.reduceByKey(_ + _)** // faz a contagem do número de vezes que cada palavra ocorreu no dataset\n",
    "\n",
    "\n",
    "**counts.saveAsTextFile(\"hdfs://...\")** // salva o RDD novamente em disco"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
